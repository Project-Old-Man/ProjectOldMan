import asyncio
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from typing import Optional

class SimpleLLMService:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.is_loaded = False
    
    async def initialize(self, model_name: str = "microsoft/DialoGPT-medium"):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForCausalLM.from_pretrained(model_name)
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            self.is_loaded = True
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.is_loaded = False
    
    async def generate_response(self, prompt: str, max_length: int = 512) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        if not self.is_loaded:
            return "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            inputs = self.tokenizer.encode(prompt, return_tensors="pt")
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + max_length,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    top_p=0.9
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = response[len(prompt):].strip()
            
            return response if response else "ì£„ì†¡í•©ë‹ˆë‹¤. ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# ì „ì—­ LLM ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
llm_service = SimpleLLMService()
