import logging
from typing import List
import os
import hashlib

logger = logging.getLogger(__name__)

class EmbeddingService:
    def __init__(self, model_name: str = "jhgan/ko-sroberta-multitask"):
        self.model_name = model_name
        self.model = None
        self.embedding_dim = 768  # ko-sroberta-multitaskì˜ ì¶œë ¥ ì°¨ì›
        self.use_mock = False  # ì‹¤ì œ ëª¨ë¸ì„ ê¸°ë³¸ìœ¼ë¡œ ë³€ê²½
        
        # í™˜ê²½ë³€ìˆ˜ë¡œ Mock ëª¨ë“œ ê°•ì œ ê°€ëŠ¥ (ë””ë²„ê¹…ìš©)
        force_mock = os.getenv("USE_MOCK_EMBEDDING", "false").lower() == "true"
        
        if force_mock:
            logger.info(f"ğŸ¤– Mock ì„ë² ë”© ëª¨ë“œ ê°•ì œ í™œì„±í™”")
            self.use_mock = True
        else:
            logger.info(f"ğŸ”„ ì‹¤ì œ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_name}")
            try:
                self._load_real_model()
                self.use_mock = False
                logger.info(f"âœ… ì‹¤ì œ ì„ë² ë”© ëª¨ë¸ í™œì„±í™” ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ ì‹¤ì œ ì„ë² ë”© ì‹¤íŒ¨, Mock ëª¨ë“œë¡œ ì „í™˜: {e}")
                logger.error(f"   ìƒì„¸ ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
                self.use_mock = True
    
    def _load_real_model(self):
        """ì‹¤ì œ ì„ë² ë”© ëª¨ë¸ ë¡œë”© - ì—¬ëŸ¬ ëª¨ë¸ ì‹œë„"""
        import numpy as np
        import torch
        
        # ì˜ì¡´ì„± ë¬¸ì œ í•´ê²° ì‹œë„
        self._fix_dependencies()
        
        # NLTK ì„¤ì¹˜ ë° ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œë„
        self._setup_nltk()
        
        # ì‚¬ìš©í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ì„ ìµœìš°ì„ ìœ¼ë¡œ)
        models_to_try = [
            "jhgan/ko-sroberta-multitask",  # í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ (768ì°¨ì›)
            "intfloat/multilingual-e5-small",  # ë‹¤êµ­ì–´ ëª¨ë¸
            "sentence-transformers/all-MiniLM-L6-v2",  # ì•ˆì •ì  ëŒ€ì•ˆ
            "sentence-transformers/paraphrase-MiniLM-L6-v2", 
            "sentence-transformers/all-mpnet-base-v2",
            "sentence-transformers/distilbert-base-nli-mean-tokens"
        ]
        
        for model_name in models_to_try:
            try:
                logger.info(f"ğŸ”„ ëª¨ë¸ ì‹œë„ ì¤‘: {model_name}")
                from sentence_transformers import SentenceTransformer
                
                # ëª¨ë¸ ë¡œë”© ì‹œ ì¶”ê°€ ì„¤ì •
                self.model = SentenceTransformer(model_name, device='cpu')
                
                # í…ŒìŠ¤íŠ¸ (normalize_embeddings=Trueë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœì í™”)
                test_embeddings = self.model.encode(["í…ŒìŠ¤íŠ¸"], convert_to_numpy=True, normalize_embeddings=True)
                self.embedding_dim = test_embeddings.shape[1]
                self.model_name = model_name  # ì‹¤ì œ ë¡œë”©ëœ ëª¨ë¸ëª…ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_name} (ì°¨ì›: {self.embedding_dim})")
                return
                
            except Exception as e:
                logger.warning(f"âš ï¸ ëª¨ë¸ {model_name} ì‹¤íŒ¨: {e}")
                logger.warning(f"   ìƒì„¸ ì˜¤ë¥˜: {type(e).__name__}")
                continue
        
        raise RuntimeError("ëª¨ë“  ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
    
    def _fix_dependencies(self):
        """ì˜ì¡´ì„± ë¬¸ì œ ìë™ í•´ê²°"""
        try:
            # tokenizers ë²„ì „ ì²´í¬
            import tokenizers
            current_version = tokenizers.__version__
            logger.info(f"ğŸ“¦ í˜„ì¬ tokenizers ë²„ì „: {current_version}")
            
            # ë²„ì „ì´ 0.13.0 ì´ìƒì´ë©´ ë‹¤ìš´ê·¸ë ˆì´ë“œ í•„ìš”
            if self._version_compare(current_version, "0.13.0") >= 0:
                logger.warning(f"âš ï¸ tokenizers {current_version}ëŠ” í˜¸í™˜ë˜ì§€ ì•ŠìŒ, ë‹¤ìš´ê·¸ë ˆì´ë“œ ì‹œë„...")
                self._downgrade_tokenizers()
            else:
                logger.info(f"âœ… tokenizers ë²„ì „ í˜¸í™˜ í™•ì¸ë¨")
                
        except ImportError:
            logger.warning("âš ï¸ tokenizersê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ, ì„¤ì¹˜ ì‹œë„...")
            self._install_compatible_tokenizers()
        except Exception as e:
            logger.warning(f"âš ï¸ tokenizers ì²´í¬ ì‹¤íŒ¨: {e}")
        
        # ì¶”ê°€ ì˜ì¡´ì„± ì²´í¬ ë° ì„¤ì¹˜
        self._install_missing_dependencies()
    
    def _install_missing_dependencies(self):
        """ëˆ„ë½ëœ ì˜ì¡´ì„± ìë™ ì„¤ì¹˜"""
        # íŒ¨í‚¤ì§€ëª…ê³¼ importëª…ì´ ë‹¤ë¥¸ ê²½ìš° ë§¤í•‘
        package_mappings = {
            'threadpoolctl': 'threadpoolctl',
            'scikit-learn': 'sklearn',  # scikit-learn -> sklearnë¡œ import
            'scipy': 'scipy',
            'numpy': 'numpy'
        }
        
        for package_name, import_name in package_mappings.items():
            try:
                __import__(import_name)
                logger.info(f"âœ… {package_name} í™•ì¸ë¨")
            except ImportError:
                logger.warning(f"âš ï¸ {package_name} ëˆ„ë½, ì„¤ì¹˜ ì‹œë„...")
                self._install_package(package_name)
    
    def _install_package(self, package_name: str):
        """ê°œë³„ íŒ¨í‚¤ì§€ ì„¤ì¹˜"""
        try:
            import subprocess
            import sys
            
            logger.info(f"ğŸ”„ {package_name} ì„¤ì¹˜ ì¤‘...")
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", package_name
            ])
            logger.info(f"âœ… {package_name} ì„¤ì¹˜ ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ {package_name} ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
    
    def _version_compare(self, version1: str, version2: str) -> int:
        """ë²„ì „ ë¹„êµ í•¨ìˆ˜ (version1 > version2ì´ë©´ 1, ê°™ìœ¼ë©´ 0, ì‘ìœ¼ë©´ -1)"""
        def version_to_tuple(v):
            return tuple(map(int, v.split('.')))
        
        v1 = version_to_tuple(version1)
        v2 = version_to_tuple(version2)
        
        if v1 > v2:
            return 1
        elif v1 < v2:
            return -1
        else:
            return 0
    
    def _downgrade_tokenizers(self):
        """tokenizersë¥¼ í˜¸í™˜ ê°€ëŠ¥í•œ ë²„ì „ìœ¼ë¡œ ë‹¤ìš´ê·¸ë ˆì´ë“œ"""
        try:
            import subprocess
            import sys
            
            # í˜¸í™˜ ê°€ëŠ¥í•œ tokenizers ë²„ì „ ì„¤ì¹˜
            compatible_version = "tokenizers==0.12.1"
            logger.info(f"ğŸ”„ {compatible_version} ì„¤ì¹˜ ì¤‘...")
            
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", 
                compatible_version, "--force-reinstall", "--no-deps"
            ])
            
            logger.info("âœ… tokenizers ë‹¤ìš´ê·¸ë ˆì´ë“œ ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ tokenizers ë‹¤ìš´ê·¸ë ˆì´ë“œ ì‹¤íŒ¨: {e}")
    
    def _install_compatible_tokenizers(self):
        """í˜¸í™˜ ê°€ëŠ¥í•œ tokenizers ì„¤ì¹˜"""
        try:
            import subprocess
            import sys
            
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", 
                "tokenizers==0.12.1"
            ])
            
            logger.info("âœ… í˜¸í™˜ ê°€ëŠ¥í•œ tokenizers ì„¤ì¹˜ ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ tokenizers ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
    
    def _setup_nltk(self):
        """NLTK ì„¤ì¹˜ ë° ì„¤ì •"""
        try:
            import nltk
            logger.info("ğŸ“š NLTK ì´ë¯¸ ì„¤ì¹˜ë¨")
            
            # í•„ìš”í•œ NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
            try:
                nltk.data.find('tokenizers/punkt')
                logger.info("âœ… NLTK punkt ë°ì´í„° í™•ì¸ë¨")
            except LookupError:
                logger.info("ğŸ”„ NLTK punkt ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
                nltk.download('punkt', quiet=True)
                
        except ImportError:
            logger.warning("âš ï¸ NLTKê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ, ì„¤ì¹˜ ì‹œë„...")
            try:
                import subprocess
                import sys
                subprocess.check_call([sys.executable, "-m", "pip", "install", "nltk"])
                
                import nltk
                nltk.download('punkt', quiet=True)
                logger.info("âœ… NLTK ì„¤ì¹˜ ë° ì„¤ì • ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ NLTK ìë™ ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
                logger.warning("   ì¼ë¶€ ëª¨ë¸ì—ì„œ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    
    def encode(self, texts: List[str]):
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        if self.use_mock:
            return self._mock_encode(texts)
        
        if not self.model:
            logger.warning("âš ï¸ ì‹¤ì œ ëª¨ë¸ì´ ì—†ì–´ Mock ëª¨ë“œë¡œ ì „í™˜")
            self.use_mock = True
            return self._mock_encode(texts)
        
        try:
            logger.info(f"ğŸ”„ ì‹¤ì œ ì„ë² ë”© ì¸ì½”ë”©: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
            # normalize_embeddings=Trueë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœì í™”
            embeddings = self.model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)
            logger.info(f"âœ… ì‹¤ì œ ì„ë² ë”© ì™„ë£Œ: {embeddings.shape}")
            return embeddings.tolist()
            
        except Exception as e:
            logger.error(f"âŒ ì‹¤ì œ ì„ë² ë”© ì‹¤íŒ¨, Mockìœ¼ë¡œ ëŒ€ì²´: {e}")
            self.use_mock = True
            return self._mock_encode(texts)
    
    def _mock_encode(self, texts: List[str]) -> List[List[float]]:
        """Mock ì„ë² ë”© ìƒì„± (í˜¸í™˜ì„± ë³´ì¥)"""
        logger.info(f"ğŸ¤– Mock ì„ë² ë”© ìƒì„±: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
        
        embeddings = []
        for text in texts:
            embedding = self._text_to_vector(text)
            embeddings.append(embedding)
        
        return embeddings
    
    def _text_to_vector(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ì¼ê´€ëœ ë²¡í„°ë¡œ ë³€í™˜ (Hash ê¸°ë°˜)"""
        # SHA-256 í•´ì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ì•ˆì •ì ì¸ ë²¡í„° ìƒì„±
        hash_obj = hashlib.sha256(text.encode('utf-8'))
        hash_bytes = hash_obj.digest()
        
        # í•´ì‹œë¥¼ float ê°’ìœ¼ë¡œ ë³€í™˜
        vector = []
        for i in range(0, len(hash_bytes), 4):
            chunk = hash_bytes[i:i+4]
            if len(chunk) == 4:
                int_val = int.from_bytes(chunk, byteorder='big')
                float_val = (int_val / (2**32 - 1)) * 2 - 1  # -1 ~ 1 ì •ê·œí™”
                vector.append(float_val)
        
        # ë²¡í„° ê¸¸ì´ë¥¼ 768ë¡œ ë§ì¶¤ (ko-sroberta-multitaskì— ë§ì¶°)
        while len(vector) < self.embedding_dim:
            # ê¸°ì¡´ ë²¡í„°ë¥¼ ë°˜ë³µí•˜ì—¬ ê¸¸ì´ ë§ì¶¤
            remaining = self.embedding_dim - len(vector)
            vector.extend(vector[:min(len(vector), remaining)])
        
        vector = vector[:self.embedding_dim]
        
        # L2 ì •ê·œí™”
        norm = sum(x*x for x in vector) ** 0.5
        if norm > 0:
            vector = [x / norm for x in vector]
        
        return vector
    
    def get_embedding_dim(self) -> int:
        """ì„ë² ë”© ì°¨ì› ìˆ˜ ë°˜í™˜"""
        return self.embedding_dim
    
    def is_using_real_model(self) -> bool:
        """ì‹¤ì œ ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ ë°˜í™˜"""
        return not self.use_mock
    
    def get_status(self) -> dict:
        """ì„ë² ë”© ì„œë¹„ìŠ¤ ìƒíƒœ ë°˜í™˜"""
        return {
            "model_name": self.model_name,
            "embedding_dim": self.embedding_dim,
            "using_real_model": not self.use_mock,
            "status": "real" if not self.use_mock else "mock",
            "compatibility_note": "ì‹¤ì œ ëª¨ë¸" if not self.use_mock else "Mock ëª¨ë“œ (ì˜ì¡´ì„± ë¬¸ì œ)"
        }

