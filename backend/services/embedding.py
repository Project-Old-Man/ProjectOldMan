import logging
from typing import List
import os
import hashlib

logger = logging.getLogger(__name__)

class EmbeddingService:
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.model = None
        self.embedding_dim = 384
        self.use_mock = True  # í˜¸í™˜ì„± ë¬¸ì œë¡œ ì¼ì‹œì  Mock ëª¨ë“œ
        
        # í™˜ê²½ë³€ìˆ˜ë¡œ ì‹¤ì œ ëª¨ë¸ ê°•ì œ ì‹œë„ ê°€ëŠ¥
        force_real = os.getenv("FORCE_REAL_EMBEDDING", "false").lower() == "true"
        
        if force_real:
            logger.info(f"ğŸ”„ ì‹¤ì œ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_name}")
            try:
                self._load_real_model()
                self.use_mock = False
            except Exception as e:
                logger.error(f"âŒ ì‹¤ì œ ì„ë² ë”© ì‹¤íŒ¨, Mock ëª¨ë“œë¡œ ì „í™˜: {e}")
                self.use_mock = True
        else:
            logger.info(f"ğŸ¤– Mock ì„ë² ë”© ëª¨ë“œë¡œ ì‹œì‘ (í˜¸í™˜ì„± ìš°ì„ )")
    
    def _load_real_model(self):
        """ì‹¤ì œ ì„ë² ë”© ëª¨ë¸ ë¡œë”©"""
        import numpy as np
        numpy_version = np.__version__
        logger.info(f"ğŸ“Š NumPy ë²„ì „: {numpy_version}")
        
        import torch
        logger.info(f"ğŸ”¥ PyTorch ë²„ì „: {torch.__version__}")
        
        from sentence_transformers import SentenceTransformer
        logger.info("ğŸ“š SentenceTransformers ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ë¨")
        
        logger.info(f"ğŸ”„ ì‹¤ì œ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
        self.model = SentenceTransformer(self.model_name)
        
        test_texts = ["í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤."]
        test_embeddings = self.model.encode(test_texts, convert_to_numpy=True)
        
        if test_embeddings is not None and len(test_embeddings) > 0:
            self.embedding_dim = test_embeddings.shape[1]
            logger.info(f"âœ… ì‹¤ì œ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì„±ê³µ! ì°¨ì›: {self.embedding_dim}")
        else:
            raise RuntimeError("í…ŒìŠ¤íŠ¸ ì¸ì½”ë”© ì‹¤íŒ¨")
    
    def encode(self, texts: List[str]):
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        if self.use_mock:
            return self._mock_encode(texts)
        
        if not self.model:
            logger.warning("âš ï¸ ì‹¤ì œ ëª¨ë¸ì´ ì—†ì–´ Mock ëª¨ë“œë¡œ ì „í™˜")
            self.use_mock = True
            return self._mock_encode(texts)
        
        try:
            logger.info(f"ğŸ”„ ì‹¤ì œ ì„ë² ë”© ì¸ì½”ë”©: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
            embeddings = self.model.encode(texts, convert_to_numpy=True)
            logger.info(f"âœ… ì‹¤ì œ ì„ë² ë”© ì™„ë£Œ: {embeddings.shape}")
            return embeddings.tolist()
            
        except Exception as e:
            logger.error(f"âŒ ì‹¤ì œ ì„ë² ë”© ì‹¤íŒ¨, Mockìœ¼ë¡œ ëŒ€ì²´: {e}")
            self.use_mock = True
            return self._mock_encode(texts)
    
    def _mock_encode(self, texts: List[str]) -> List[List[float]]:
        """Mock ì„ë² ë”© ìƒì„± (í˜¸í™˜ì„± ë³´ì¥)"""
        logger.info(f"ğŸ¤– Mock ì„ë² ë”© ìƒì„±: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
        
        embeddings = []
        for text in texts:
            embedding = self._text_to_vector(text)
            embeddings.append(embedding)
        
        return embeddings
    
    def _text_to_vector(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ì¼ê´€ëœ ë²¡í„°ë¡œ ë³€í™˜ (Hash ê¸°ë°˜)"""
        # SHA-256 í•´ì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ì•ˆì •ì ì¸ ë²¡í„° ìƒì„±
        hash_obj = hashlib.sha256(text.encode('utf-8'))
        hash_bytes = hash_obj.digest()
        
        # í•´ì‹œë¥¼ float ê°’ìœ¼ë¡œ ë³€í™˜
        vector = []
        for i in range(0, len(hash_bytes), 4):
            chunk = hash_bytes[i:i+4]
            if len(chunk) == 4:
                int_val = int.from_bytes(chunk, byteorder='big')
                float_val = (int_val / (2**32 - 1)) * 2 - 1  # -1 ~ 1 ì •ê·œí™”
                vector.append(float_val)
        
        # ë²¡í„° ê¸¸ì´ë¥¼ 384ë¡œ ë§ì¶¤
        while len(vector) < self.embedding_dim:
            # ê¸°ì¡´ ë²¡í„°ë¥¼ ë°˜ë³µí•˜ì—¬ ê¸¸ì´ ë§ì¶¤
            remaining = self.embedding_dim - len(vector)
            vector.extend(vector[:min(len(vector), remaining)])
        
        vector = vector[:self.embedding_dim]
        
        # L2 ì •ê·œí™”
        norm = sum(x*x for x in vector) ** 0.5
        if norm > 0:
            vector = [x / norm for x in vector]
        
        return vector
    
    def get_embedding_dim(self) -> int:
        """ì„ë² ë”© ì°¨ì› ìˆ˜ ë°˜í™˜"""
        return self.embedding_dim
    
    def is_using_real_model(self) -> bool:
        """ì‹¤ì œ ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ ë°˜í™˜"""
        return not self.use_mock
    
    def get_status(self) -> dict:
        """ì„ë² ë”© ì„œë¹„ìŠ¤ ìƒíƒœ ë°˜í™˜"""
        return {
            "model_name": self.model_name,
            "embedding_dim": self.embedding_dim,
            "using_real_model": not self.use_mock,
            "status": "real" if not self.use_mock else "mock",
            "compatibility_note": "Mock ëª¨ë“œ (PyTorch í˜¸í™˜ì„± ë¬¸ì œ)" if self.use_mock else "ì‹¤ì œ ëª¨ë¸"
        }
