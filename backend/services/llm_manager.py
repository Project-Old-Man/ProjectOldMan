import os
import asyncio
import logging
from typing import Optional, Dict, Any
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LLMManager:
    def __init__(self, model_path: str = "models/tinyllama.gguf"):
        self.model_path = model_path
        self.llm = None
        self.use_mock = True  # ê¸°ë³¸ì ìœ¼ë¡œ mock ëª¨ë“œë¡œ ì‹œì‘
        self.model_info = {
            "name": "TinyLlama GGUF",
            "type": "gguf",
            "status": "initializing",
            "path": model_path,
            "loaded": False,
            "error_message": None
        }
        
        # ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë”© ì‹œë„
        self.load_local_model()
    
    def load_local_model(self) -> bool:
        """ë¡œì»¬ GGUF ëª¨ë¸ ë¡œë”© ì‹œë„"""
        try:
            logger.info(f"ğŸ¤– ëª¨ë¸ ë¡œë”© ì‹œë„: {self.model_path}")
            
            # 1. íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(self.model_path):
                raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
            
            # 2. íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = os.path.getsize(self.model_path) / (1024 * 1024)  # MB
            logger.info(f"ğŸ“ ëª¨ë¸ íŒŒì¼ í¬ê¸°: {file_size:.1f}MB")
            
            # 3. llama-cpp-python ì„í¬íŠ¸ ì‹œë„
            try:
                from llama_cpp import Llama
                logger.info("âœ… llama-cpp-python ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ë¨")
            except ImportError as e:
                logger.error(f"âŒ llama-cpp-python ì„¤ì¹˜ í•„ìš”")
                logger.error("ğŸ’¡ ì„¤ì¹˜ ëª…ë ¹ì–´: pip install llama-cpp-python")
                logger.error("ğŸ’¡ ë˜ëŠ” CPU ìµœì í™”: pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu")
                raise ImportError("llama-cpp-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìœ„ì˜ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
            
            # 4. ëª¨ë¸ ë¡œë”© (CPU ìµœì í™” ì„¤ì • - ì„±ëŠ¥ ê°œì„ )
            logger.info("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘... (CPU ëª¨ë“œ, ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
            
            try:
                self.llm = Llama(
                    model_path=self.model_path,
                    n_ctx=4096,           # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¦ê°€ (ëª¨ë¸ í›ˆë ¨ í¬ê¸°ì— ë§ì¶¤)
                    n_threads=4,          # CPU ìŠ¤ë ˆë“œ ìˆ˜
                    n_batch=512,          # ë°°ì¹˜ í¬ê¸°
                    verbose=False,        # ìƒì„¸ ë¡œê·¸ ë¹„í™œì„±í™”
                    use_mlock=False,      # ë©”ëª¨ë¦¬ ì ê¸ˆ ë¹„í™œì„±í™” (í˜¸í™˜ì„±)
                    use_mmap=True,        # ë©”ëª¨ë¦¬ ë§µ ì‚¬ìš© (ì„±ëŠ¥ í–¥ìƒ)
                    n_gpu_layers=0,       # GPU ì‚¬ìš© ì•ˆí•¨ (CPU ì „ìš©)
                    rope_scaling_type=None,  # RoPE ìŠ¤ì¼€ì¼ë§ ë¹„í™œì„±í™”
                    logits_all=False,     # ë©”ëª¨ë¦¬ ì ˆì•½
                )
                logger.info("âœ… GGUF ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            except Exception as load_error:
                logger.error(f"âŒ GGUF ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {load_error}")
                raise load_error
            
            # 5. í…ŒìŠ¤íŠ¸ ì¶”ë¡ ìœ¼ë¡œ ëª¨ë¸ ê²€ì¦
            logger.info("ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¶”ë¡  ì¤‘...")
            try:
                test_response = self.llm(
                    "Hello",
                    max_tokens=10,
                    temperature=0.1,
                    echo=False
                )
                
                if test_response and test_response.get('choices'):
                    test_text = test_response['choices'][0]['text'].strip()
                    logger.info(f"âœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ! ì‘ë‹µ: '{test_text}'")
                    
                    self.use_mock = False
                    self.model_info.update({
                        "status": "loaded",
                        "loaded": True,
                        "error_message": None,
                        "file_size_mb": round(file_size, 1),
                        "test_response": test_text
                    })
                    return True
                else:
                    raise RuntimeError("ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                    
            except Exception as test_error:
                logger.error(f"âŒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {test_error}")
                raise test_error
                
        except FileNotFoundError as e:
            logger.warning(f"âš ï¸ {e}")
            self.model_info.update({
                "status": "file_not_found",
                "error_message": str(e)
            })
            
        except ImportError as e:
            logger.warning(f"âš ï¸ {e}")
            self.model_info.update({
                "status": "dependency_missing", 
                "error_message": str(e)
            })
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.model_info.update({
                "status": "load_error",
                "error_message": str(e)
            })
        
        # Fallback to mock mode
        logger.info("ğŸ”„ Mock ëª¨ë“œë¡œ ì „í™˜")
        self.model_info.update({
            "name": "Mock AI Assistant",
            "type": "mock"
        })
        return False
    
    async def generate_response(self, prompt: str, max_tokens: int = 256) -> str:
        """ë©”ì¸ ì‘ë‹µ ìƒì„± í•¨ìˆ˜"""
        if self.use_mock:
            return await self._generate_mock_response(prompt)
        
        if not self.llm:
            logger.error("âŒ LLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return await self._generate_mock_response(prompt)
        
        try:
            # ë¹„ë™ê¸° ì‹¤í–‰ (CPU ì§‘ì•½ì  ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ)
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None, 
                self._generate_sync, 
                prompt, 
                max_tokens
            )
            return response
            
        except Exception as e:
            logger.error(f"âŒ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _generate_sync(self, prompt: str, max_tokens: int) -> str:
        """ë™ê¸° ì¶”ë¡  í•¨ìˆ˜ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë¨)"""
        try:
            logger.info(f"ğŸ¤– ì¶”ë¡  ì‹œì‘ (max_tokens: {max_tokens})")
            
            # í•œêµ­ì–´ ëŒ€í™” í˜•ì‹ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            formatted_prompt = f"""ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ì…ë‹ˆë‹¤. ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì: {prompt}
AI ì–´ì‹œìŠ¤í„´íŠ¸:"""
            
            # ì¶”ë¡  ì‹¤í–‰
            output = self.llm(
                formatted_prompt,
                max_tokens=max_tokens,
                temperature=0.7,          # ì°½ì˜ì„± ì¡°ì ˆ
                top_p=0.9,               # í† í° ì„ íƒ ë‹¤ì–‘ì„±
                top_k=40,                # ìƒìœ„ Kê°œ í† í°ë§Œ ê³ ë ¤
                repeat_penalty=1.1,      # ë°˜ë³µ ë°©ì§€
                echo=False,              # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œì™¸
                stop=["ì‚¬ìš©ì:", "User:", "\n\n", "AI ì–´ì‹œìŠ¤í„´íŠ¸:", "Human:"]  # ì¤‘ë‹¨ í† í°
            )
            
            if output and output.get('choices') and len(output['choices']) > 0:
                response_text = output['choices'][0]['text'].strip()
                
                # ì‘ë‹µ í›„ì²˜ë¦¬
                response_text = self._clean_response(response_text)
                
                logger.info(f"âœ… ì¶”ë¡  ì™„ë£Œ (ì‘ë‹µ ê¸¸ì´: {len(response_text)}ì)")
                return response_text
            else:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."
                
        except Exception as e:
            logger.error(f"âŒ ë™ê¸° ì¶”ë¡  ì˜¤ë¥˜: {e}")
            return f"ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _clean_response(self, text: str) -> str:
        """ì‘ë‹µ í…ìŠ¤íŠ¸ ì •ë¦¬"""
        # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
        text = text.strip()
        
        # ì¤‘ë³µ ì¤„ë°”ê¿ˆ ì œê±°
        import re
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        # ë„ˆë¬´ ê¸´ ì‘ë‹µ ìë¥´ê¸°
        if len(text) > 500:
            text = text[:500] + "..."
        
        return text
    
    async def _generate_mock_response(self, prompt: str) -> str:
        """Mock ì‘ë‹µ ìƒì„±"""
        await asyncio.sleep(0.5)  # ì‹¤ì œ ì¶”ë¡  ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
        
        mock_prefix = "ğŸ¤– [Mock AI] "
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì‘ë‹µ
        prompt_lower = prompt.lower()
        
        if any(keyword in prompt_lower for keyword in ["ê±´ê°•", "health", "í˜ˆì••", "ë‹¹ë‡¨", "ìš´ë™"]):
            return f"{mock_prefix}ê±´ê°•ì— ê´€í•œ ì§ˆë¬¸ì´ì‹œêµ°ìš”. ê· í˜• ì¡íŒ ì‹ë‹¨ê³¼ ê·œì¹™ì ì¸ ìš´ë™ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì¦ìƒì´ ìˆë‹¤ë©´ ì˜ë£Œì§„ê³¼ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
            
        elif any(keyword in prompt_lower for keyword in ["ì—¬í–‰", "travel", "ì œì£¼ë„", "ë¶€ì‚°", "ê´€ê´‘"]):
            return f"{mock_prefix}ì—¬í–‰ ê³„íšì„ ì„¸ìš°ê³  ê³„ì‹œëŠ”êµ°ìš”! ëª©ì ì§€ì˜ ë‚ ì”¨, í˜„ì§€ ë¬¸í™”, í•„ìˆ˜ ì„œë¥˜ ë“±ì„ ë¯¸ë¦¬ í™•ì¸í•˜ì‹œê³  ì•ˆì „í•œ ì—¬í–‰ ë˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
            
        elif any(keyword in prompt_lower for keyword in ["íˆ¬ì", "investment", "ì£¼ì‹", "ë¶€ë™ì‚°", "ì—°ê¸ˆ"]):
            return f"{mock_prefix}íˆ¬ìì™€ ì¬ì • ê´€ë¦¬ì— ëŒ€í•´ ë¬¸ì˜í•˜ì…¨êµ°ìš”. ë¶„ì‚° íˆ¬ìì™€ ì¥ê¸°ì  ê´€ì ì´ ì¤‘ìš”í•˜ë©°, ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì‹œëŠ” ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤."
            
        elif any(keyword in prompt_lower for keyword in ["ë²•ë¥ ", "legal", "ê³„ì•½", "ìƒì†", "ë³€í˜¸ì‚¬"]):
            return f"{mock_prefix}ë²•ë¥  ê´€ë ¨ ë¬¸ì˜ì‚¬í•­ì´êµ°ìš”. ì¼ë°˜ì ì¸ ì •ë³´ëŠ” ì œê³µë“œë¦´ ìˆ˜ ìˆì§€ë§Œ, êµ¬ì²´ì ì¸ ì‚¬ì•ˆì€ ë³€í˜¸ì‚¬ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
            
        else:
            return f"{mock_prefix}ì•ˆë…•í•˜ì„¸ìš”! ì§ˆë¬¸ì— ëŒ€í•´ ë„ì›€ì„ ë“œë¦¬ê³  ì‹¶ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    
    def get_model_info(self) -> Dict[str, Any]:
        """í˜„ì¬ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            **self.model_info,
            "loaded": self.llm is not None,
            "available_models": self._scan_available_models(),
            "is_mock": self.use_mock
        }
    
    def _scan_available_models(self) -> list:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ íŒŒì¼ ìŠ¤ìº”"""
        models_dir = Path("models")
        available_models = []
        
        if models_dir.exists():
            for file_path in models_dir.glob("*"):
                if file_path.suffix.lower() in ['.gguf', '.bin', '.safetensors']:
                    try:
                        size_mb = file_path.stat().st_size / (1024 * 1024)
                        available_models.append({
                            "name": file_path.name,
                            "path": str(file_path),
                            "size_mb": round(size_mb, 1),
                            "extension": file_path.suffix
                        })
                    except Exception as e:
                        logger.warning(f"íŒŒì¼ ì •ë³´ ì½ê¸° ì‹¤íŒ¨: {file_path} - {e}")
        
        return available_models
    
    def reload_model(self, new_model_path: Optional[str] = None) -> bool:
        """ëª¨ë¸ ì¬ë¡œë”©"""
        if new_model_path:
            self.model_path = new_model_path
            self.model_info["path"] = new_model_path
        
        # ê¸°ì¡´ ëª¨ë¸ í•´ì œ
        if self.llm:
            del self.llm
            self.llm = None
        
        return self.load_local_model()

# ì „ì—­ LLM ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
_llm_manager_instance = None

def get_llm_manager() -> LLMManager:
    """ì‹±ê¸€í†¤ LLM ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _llm_manager_instance
    if _llm_manager_instance is None:
        _llm_manager_instance = LLMManager()
    return _llm_manager_instance
