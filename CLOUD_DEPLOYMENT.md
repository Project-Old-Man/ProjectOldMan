# â˜ï¸ í´ë¼ìš°ë“œ ë°°í¬ ê°€ì´ë“œ

## ğŸ¯ **ë°°í¬ ì „ëµ**

```
ğŸŒ Frontend (Vercel) â†’ â˜ï¸ Backend (Cloud) â†’ ğŸ¤– Model (Colab Pro)
```

---

## ğŸ¨ **1ë‹¨ê³„: Frontend Vercel ë°°í¬**

### **1-1. Vercel í”„ë¡œì íŠ¸ ì¤€ë¹„**

#### **í”„ë¡œì íŠ¸ êµ¬ì¡° ìˆ˜ì •**
```bash
# frontend ë””ë ‰í† ë¦¬ë¥¼ ë£¨íŠ¸ë¡œ ì´ë™
mv frontend/* .
rmdir frontend

# vercel.json ì„¤ì • íŒŒì¼ ìƒì„±
```

#### **vercel.json ìƒì„±**
```json
{
  "version": 2,
  "builds": [
    {
      "src": "index.html",
      "use": "@vercel/static"
    }
  ],
  "routes": [
    {
      "src": "/(.*)",
      "dest": "/index.html"
    }
  ],
  "env": {
    "BACKEND_URL": "https://your-backend-url.com"
  }
}
```

#### **í™˜ê²½ë³„ ë°±ì—”ë“œ URL ì„¤ì •**
```javascript
// index.htmlì—ì„œ ë°±ì—”ë“œ URL ë™ì  ì„¤ì •
const BACKEND_URL = process.env.BACKEND_URL || 'http://localhost:8000';

// API í˜¸ì¶œ ë¶€ë¶„ ìˆ˜ì •
const response = await fetch(`${BACKEND_URL}/query`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ question: message })
});
```

### **1-2. Vercel ë°°í¬**

#### **GitHub ì—°ë™ ë°°í¬ (ê¶Œì¥)**
```bash
# 1. GitHubì— ì½”ë“œ í‘¸ì‹œ
git add .
git commit -m "Add Vercel deployment config"
git push origin main

# 2. Vercelì—ì„œ GitHub ì €ì¥ì†Œ ì—°ê²°
# - vercel.com ì ‘ì†
# - "New Project" í´ë¦­
# - GitHub ì €ì¥ì†Œ ì„ íƒ
# - í™˜ê²½ ë³€ìˆ˜ ì„¤ì •: BACKEND_URL=https://your-backend-url.com
```

#### **CLI ë°°í¬**
```bash
# 1. Vercel CLI ì„¤ì¹˜
npm i -g vercel

# 2. ë¡œê·¸ì¸
vercel login

# 3. ë°°í¬
vercel --prod
```

### **1-3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**
```bash
# Vercel ëŒ€ì‹œë³´ë“œì—ì„œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
BACKEND_URL=https://your-backend-url.com
```

---

## â˜ï¸ **2ë‹¨ê³„: Backend í´ë¼ìš°ë“œ ë°°í¬**

### **2-1. ì˜µì…˜ A: Railway (ê¶Œì¥ - ê°„ë‹¨)**

#### **Railway ë°°í¬**
```bash
# 1. Railway CLI ì„¤ì¹˜
npm install -g @railway/cli

# 2. ë¡œê·¸ì¸
railway login

# 3. í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
cd backend
railway init

# 4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
railway variables set DATABASE_URL="postgresql://..."
railway variables set WEAVIATE_URL="https://your-weaviate-url"
railway variables set COLAB_API_URL="https://your-colab-url.ngrok.io"

# 5. ë°°í¬
railway up
```

#### **Railway ì„¤ì • íŒŒì¼**
```json
// railway.json
{
  "build": {
    "builder": "DOCKERFILE"
  },
  "deploy": {
    "startCommand": "uvicorn main:app --host 0.0.0.0 --port $PORT",
    "healthcheckPath": "/health",
    "healthcheckTimeout": 300,
    "restartPolicyType": "ON_FAILURE"
  }
}
```

### **2-2. ì˜µì…˜ B: Render**

#### **Render ë°°í¬**
```bash
# 1. render.yaml ìƒì„±
services:
  - type: web
    name: ai-playground-backend
    env: python
    buildCommand: pip install -r requirements.txt
    startCommand: uvicorn main:app --host 0.0.0.0 --port $PORT
    envVars:
      - key: DATABASE_URL
        value: postgresql://...
      - key: WEAVIATE_URL
        value: https://your-weaviate-url
      - key: COLAB_API_URL
        value: https://your-colab-url.ngrok.io
```

### **2-3. ì˜µì…˜ C: Heroku**

#### **Heroku ë°°í¬**
```bash
# 1. Heroku CLI ì„¤ì¹˜ ë° ë¡œê·¸ì¸
heroku login

# 2. ì•± ìƒì„±
heroku create ai-playground-backend

# 3. PostgreSQL ì¶”ê°€
heroku addons:create heroku-postgresql:hobby-dev

# 4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
heroku config:set DATABASE_URL="postgresql://..."
heroku config:set WEAVIATE_URL="https://your-weaviate-url"
heroku config:set COLAB_API_URL="https://your-colab-url.ngrok.io"

# 5. ë°°í¬
git push heroku main
```

### **2-4. ì˜µì…˜ D: Google Cloud Run**

#### **Cloud Run ë°°í¬**
```bash
# 1. Dockerfile ìµœì í™”
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .
EXPOSE 8080

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8080"]

# 2. ë¹Œë“œ ë° ë°°í¬
gcloud builds submit --tag gcr.io/PROJECT_ID/ai-backend
gcloud run deploy ai-backend --image gcr.io/PROJECT_ID/ai-backend --platform managed
```

---

## ğŸ¤– **3ë‹¨ê³„: Colab Pro ëª¨ë¸ ì„œë¹™**

### **3-1. Colab Proì—ì„œ ì„œë²„ ì‹¤í–‰**

#### **Colab ë…¸íŠ¸ë¶ ì½”ë“œ**
```python
# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
!pip install fastapi uvicorn transformers torch pyngrok

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from pyngrok import ngrok
import json

app = FastAPI()

# CORS ì„¤ì • (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì ‘ê·¼ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ëª¨ë¸ ë¡œë“œ
print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
model_name = "microsoft/DialoGPT-medium"  # ì›í•˜ëŠ” ëª¨ë¸ë¡œ ë³€ê²½
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

@app.get("/health")
async def health_check():
    return {"status": "healthy", "model": model_name}

@app.post("/generate")
async def generate_text(request: dict):
    try:
        prompt = request["prompt"]
        temperature = request.get("temperature", 0.7)
        max_tokens = request.get("max_tokens", 512)
        
        # í† í°í™”
        inputs = tokenizer.encode(prompt, return_tensors="pt")
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=inputs.shape[1] + max_tokens,
                temperature=temperature,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                top_p=0.9
            )
        
        # ë””ì½”ë”©
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response[len(prompt):]  # í”„ë¡¬í”„íŠ¸ ì œê±°
        
        return {"response": response, "model": model_name}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/stream")
async def generate_stream(request: dict):
    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (Server-Sent Events)
    from fastapi.responses import StreamingResponse
    import asyncio
    
    async def generate():
        try:
            prompt = request["prompt"]
            temperature = request.get("temperature", 0.7)
            max_tokens = request.get("max_tokens", 512)
            
            inputs = tokenizer.encode(prompt, return_tensors="pt")
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_length=inputs.shape[1] + max_tokens,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                    top_p=0.9,
                    return_dict_in_generate=True,
                    output_scores=True
                )
            
            response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)
            response = response[len(prompt):]
            
            # ë‹¨ì–´ë³„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
            words = response.split()
            for i, word in enumerate(words):
                yield f"data: {json.dumps({'chunk': word + (' ' if i < len(words) - 1 else '')})}\n\n"
                await asyncio.sleep(0.05)
            
            yield f"data: {json.dumps({'type': 'done'})}\n\n"
            
        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return StreamingResponse(generate(), media_type="text/plain")

# ngrok í„°ë„ ìƒì„±
print("ğŸŒ ngrok í„°ë„ ìƒì„± ì¤‘...")
public_url = ngrok.connect(8000)
print(f"âœ… Public URL: {public_url}")

# ì„œë²„ ì‹¤í–‰
print("ğŸš€ ì„œë²„ ì‹œì‘...")
import uvicorn
uvicorn.run(app, host="0.0.0.0", port=8000)
```

### **3-2. ë°±ì—”ë“œì—ì„œ Colab API ì—°ê²°**

#### **í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**
```bash
# ë°±ì—”ë“œ í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ
COLAB_API_URL=https://your-ngrok-url.ngrok.io
```

#### **ëª¨ë¸ ì„¤ì • ì—…ë°ì´íŠ¸**
```json
{
  "models": {
    "colab-model": {
      "name": "colab-api",
      "type": "colab",
      "description": "Colab Proì—ì„œ ì‹¤í–‰í•˜ëŠ” ëª¨ë¸",
      "parameters": {
        "max_tokens": 1024,
        "temperature": 0.7,
        "top_p": 0.9
      },
      "supported_tasks": ["text-generation", "chat"],
      "language": "ko",
      "domain": "general"
    }
  }
}
```

---

## ğŸ”§ **4ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •**

### **4-1. PostgreSQL í´ë¼ìš°ë“œ ì„¤ì •**

#### **Railway PostgreSQL**
```bash
# Railwayì—ì„œ PostgreSQL ì¶”ê°€
railway add postgresql

# ì—°ê²° ì •ë³´ í™•ì¸
railway variables
```

#### **Supabase (ë¬´ë£Œ ëŒ€ì•ˆ)**
```bash
# 1. supabase.comì—ì„œ í”„ë¡œì íŠ¸ ìƒì„±
# 2. ì—°ê²° ì •ë³´ ë³µì‚¬
DATABASE_URL=postgresql://postgres:[password]@db.[project].supabase.co:5432/postgres
```

### **4-2. Weaviate í´ë¼ìš°ë“œ ì„¤ì •**

#### **Weaviate Cloud Services**
```bash
# 1. weaviate.ioì—ì„œ í´ëŸ¬ìŠ¤í„° ìƒì„±
# 2. ì—°ê²° ì •ë³´ ì„¤ì •
WEAVIATE_URL=https://your-cluster.weaviate.network
WEAVIATE_API_KEY=your-api-key
```

---

## ğŸ”— **5ë‹¨ê³„: ì „ì²´ ì—°ê²° í…ŒìŠ¤íŠ¸**

### **5-1. ì—°ê²° í™•ì¸**
```bash
# 1. Frontend í™•ì¸
curl https://your-vercel-app.vercel.app

# 2. Backend í™•ì¸
curl https://your-backend-url.com/health

# 3. Colab API í™•ì¸
curl -X POST "https://your-colab-url.ngrok.io/health"

# 4. ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
curl -X POST "https://your-backend-url.com/query" \
     -H "Content-Type: application/json" \
     -d '{"question": "ì•ˆë…•í•˜ì„¸ìš”"}'
```

### **5-2. í™˜ê²½ ë³€ìˆ˜ ìµœì¢… ì„¤ì •**

#### **Vercel (Frontend)**
```bash
BACKEND_URL=https://your-backend-url.com
```

#### **Railway/Heroku (Backend)**
```bash
DATABASE_URL=postgresql://...
WEAVIATE_URL=https://your-weaviate-url
COLAB_API_URL=https://your-colab-url.ngrok.io
CORS_ORIGINS=https://your-vercel-app.vercel.app
```

---

## ğŸ“Š **ë¹„ìš© ë¹„êµ**

| ì„œë¹„ìŠ¤ | ì›” ë¹„ìš© | íŠ¹ì§• |
|--------|---------|------|
| **Vercel** | $0 (Hobby) | í”„ë¡ íŠ¸ì—”ë“œ ë¬´ë£Œ |
| **Railway** | $5 (Starter) | ë°±ì—”ë“œ ê°„ë‹¨ ë°°í¬ |
| **Render** | $7 (Starter) | ë°±ì—”ë“œ ì•ˆì •ì  |
| **Heroku** | $7 (Basic) | ë°±ì—”ë“œ ì „í†µì  |
| **Colab Pro** | $10 | GPU ëª¨ë¸ ì„œë¹™ |
| **Supabase** | $0 (Free) | PostgreSQL ë¬´ë£Œ |
| **Weaviate Cloud** | $25 (Starter) | ë²¡í„° DB |

**ì´ ì›” ë¹„ìš©: $15-40**

---

## ğŸš€ **ì¶”ì²œ ë°°í¬ ìˆœì„œ**

### **1ë‹¨ê³„: Colab Pro ì„¤ì •**
```bash
# Colabì—ì„œ ëª¨ë¸ ì„œë²„ ì‹¤í–‰
# ngrok URL ë³µì‚¬
```

### **2ë‹¨ê³„: Backend ë°°í¬**
```bash
# Railway ì„ íƒ (ê°€ì¥ ê°„ë‹¨)
railway up
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```

### **3ë‹¨ê³„: Frontend ë°°í¬**
```bash
# Vercel ë°°í¬
vercel --prod
# BACKEND_URL í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```

### **4ë‹¨ê³„: ì—°ê²° í…ŒìŠ¤íŠ¸**
```bash
# ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
```

---

## âš ï¸ **ì£¼ì˜ì‚¬í•­**

### **Colab Pro**
- **ì„¸ì…˜ ì‹œê°„**: 12ì‹œê°„ ì œí•œ
- **ì¬ì—°ê²°**: ngrok URL ë³€ê²½ ì‹œ ë°±ì—”ë“œ í™˜ê²½ ë³€ìˆ˜ ì—…ë°ì´íŠ¸ í•„ìš”
- **ë¹„ìš©**: ì›” $10

### **í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤**
- **ë¬´ë£Œ í‹°ì–´**: ì‚¬ìš©ëŸ‰ ì œí•œ í™•ì¸
- **ìŠ¤ì¼€ì¼ë§**: íŠ¸ë˜í”½ ì¦ê°€ ì‹œ ìë™ ìŠ¤ì¼€ì¼ë§ ì„¤ì •
- **ëª¨ë‹ˆí„°ë§**: ë¡œê·¸ ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì •

---

## ğŸ‰ **ì™„ì„±ëœ ì•„í‚¤í…ì²˜**

```
ğŸŒ Frontend (Vercel)
    â†“
â˜ï¸ Backend (Railway/Heroku)
    â†“
ğŸ¤– Model (Colab Pro)
    â†“
ğŸ’¾ Database (Supabase/Weaviate)
```

**ì´ì œ ì™„ì „í•œ í´ë¼ìš°ë“œ ê¸°ë°˜ AI ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì™„ì„±ë©ë‹ˆë‹¤! ğŸš€** 