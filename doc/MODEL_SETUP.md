# ğŸ¤– AI ëª¨ë¸ ì„¤ì • ê°€ì´ë“œ

## ğŸ¯ **ëª¨ë¸ ì—°ê²° ë°©ë²• 2ê°€ì§€**

### **ë°©ë²• 1: ë¡œì»¬ ëª¨ë¸ íŒŒì¼ ì‚¬ìš©**
### **ë°©ë²• 2: Colab Pro API ì—°ê²°**

---

## ğŸ“ **ë°©ë²• 1: ë¡œì»¬ ëª¨ë¸ íŒŒì¼ ì‚¬ìš©**

### **1ë‹¨ê³„: ëª¨ë¸ íŒŒì¼ ì¤€ë¹„**

#### **ì§€ì›í•˜ëŠ” ëª¨ë¸ í˜•ì‹**
- âœ… **HuggingFace Transformers**: `.bin`, `.safetensors`
- âœ… **GGUF**: `.gguf` (vLLM 0.3.0+)
- âœ… **AWQ**: `.awq`
- âœ… **GPTQ**: `.gptq`

#### **ëª¨ë¸ íŒŒì¼ êµ¬ì¡°**
```
model/local/my-korean-model/
â”œâ”€â”€ config.json              # ëª¨ë¸ ì„¤ì •
â”œâ”€â”€ tokenizer.json           # í† í¬ë‚˜ì´ì €
â”œâ”€â”€ tokenizer_config.json    # í† í¬ë‚˜ì´ì € ì„¤ì •
â”œâ”€â”€ model.safetensors        # ëª¨ë¸ ê°€ì¤‘ì¹˜ (ëŒ€ìš©ëŸ‰)
â”œâ”€â”€ generation_config.json   # ìƒì„± ì„¤ì •
â””â”€â”€ README.md               # ëª¨ë¸ ì„¤ëª…
```

### **2ë‹¨ê³„: ëª¨ë¸ íŒŒì¼ ë³µì‚¬**
```bash
# ëª¨ë¸ íŒŒì¼ì„ model/local/ ë””ë ‰í† ë¦¬ì— ë³µì‚¬
cp -r /path/to/your/model model/local/my-korean-model

# ì˜ˆì‹œ: í•œêµ­ì–´ ëª¨ë¸
cp -r ~/Downloads/korean-llama-2-7b model/local/korean-llama
```

### **3ë‹¨ê³„: ëª¨ë¸ ì„¤ì • ì¶”ê°€**
```bash
# model/models.json íŒŒì¼ì— ëª¨ë¸ ì •ë³´ ì¶”ê°€
{
  "models": {
    "my-korean-model": {
      "name": "./model/local/my-korean-model",
      "type": "local",
      "description": "ë‚´ê°€ ë§Œë“  í•œêµ­ì–´ ëª¨ë¸",
      "parameters": {
        "max_tokens": 1024,
        "temperature": 0.7,
        "top_p": 0.9
      },
      "supported_tasks": ["text-generation", "chat"],
      "language": "ko",
      "domain": "general"
    }
  }
}
```

### **4ë‹¨ê³„: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**
```bash
# .env íŒŒì¼ì—ì„œ
LLM_MODEL=./model/local/my-korean-model
```

### **5ë‹¨ê³„: ì„œë²„ ì¬ì‹œì‘**
```bash
# ë°±ì—”ë“œ ì¬ì‹œì‘
cd backend
python3 -m uvicorn main:app --reload
```

---

## â˜ï¸ **ë°©ë²• 2: Colab Pro API ì—°ê²°**

### **1ë‹¨ê³„: Colab Proì—ì„œ ëª¨ë¸ ì„œë¹™**

#### **Colab ë…¸íŠ¸ë¶ì—ì„œ FastAPI ì„œë²„ ì‹¤í–‰**
```python
# Colab ë…¸íŠ¸ë¶ì— ë‹¤ìŒ ì½”ë“œ ì‘ì„±
!pip install fastapi uvicorn transformers torch

from fastapi import FastAPI
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

app = FastAPI()

# ëª¨ë¸ ë¡œë“œ
model_name = "your-model-name"  # ì˜ˆ: "microsoft/DialoGPT-medium"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

@app.post("/generate")
async def generate_text(request: dict):
    prompt = request["prompt"]
    temperature = request.get("temperature", 0.7)
    max_tokens = request.get("max_tokens", 512)
    
    # í† í°í™”
    inputs = tokenizer.encode(prompt, return_tensors="pt")
    
    # ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_length=inputs.shape[1] + max_tokens,
            temperature=temperature,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # ë””ì½”ë”©
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response[len(prompt):]  # í”„ë¡¬í”„íŠ¸ ì œê±°
    
    return {"response": response}

# ì„œë²„ ì‹¤í–‰
import uvicorn
uvicorn.run(app, host="0.0.0.0", port=8000)
```

#### **ngrokìœ¼ë¡œ ì™¸ë¶€ ì ‘ì† ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ê¸°**
```python
# Colabì—ì„œ ngrok ì„¤ì¹˜ ë° ì‹¤í–‰
!pip install pyngrok
from pyngrok import ngrok

# í„°ë„ ìƒì„±
public_url = ngrok.connect(8000)
print(f"Public URL: {public_url}")
```

### **2ë‹¨ê³„: ë°±ì—”ë“œì—ì„œ Colab API ì—°ê²°**

#### **í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**
```bash
# .env íŒŒì¼ì— ì¶”ê°€
COLAB_API_URL=https://your-ngrok-url.ngrok.io
```

#### **ëª¨ë¸ ì„¤ì • ì¶”ê°€**
```json
{
  "models": {
    "colab-model": {
      "name": "colab-api",
      "type": "colab",
      "description": "Colab Proì—ì„œ ì‹¤í–‰í•˜ëŠ” ëª¨ë¸",
      "parameters": {
        "max_tokens": 1024,
        "temperature": 0.7,
        "top_p": 0.9
      },
      "supported_tasks": ["text-generation", "chat"],
      "language": "ko",
      "domain": "general"
    }
  }
}
```

### **3ë‹¨ê³„: API ì—°ê²° í…ŒìŠ¤íŠ¸**
```bash
# Colab API í…ŒìŠ¤íŠ¸
curl -X POST "https://your-ngrok-url.ngrok.io/generate" \
     -H "Content-Type: application/json" \
     -d '{"prompt": "ì•ˆë…•í•˜ì„¸ìš”", "max_tokens": 100}'
```

---

## ğŸ”§ **ëª¨ë¸ ì „í™˜ ë°©ë²•**

### **APIë¡œ ëª¨ë¸ ì „í™˜**
```bash
# ë¡œì»¬ ëª¨ë¸ë¡œ ì „í™˜
curl -X POST "http://localhost:8000/admin/switch-model" \
     -H "Content-Type: application/json" \
     -d '{"model_id": "my-korean-model"}'

# Colab ëª¨ë¸ë¡œ ì „í™˜
curl -X POST "http://localhost:8000/admin/switch-model" \
     -H "Content-Type: application/json" \
     -d '{"model_id": "colab-model"}'
```

### **í™˜ê²½ ë³€ìˆ˜ë¡œ ì „í™˜**
```bash
# ë¡œì»¬ ëª¨ë¸
export LLM_MODEL=./model/local/my-korean-model

# Colab API
export LLM_MODEL=colab-api
```

---

## ğŸ“Š **ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ**

### **ë¡œì»¬ ëª¨ë¸**
- âœ… **ì¥ì **: ë¹ ë¥¸ ì‘ë‹µ, ì˜¤í”„ë¼ì¸ ì‘ë™, ë¹„ìš© ì—†ìŒ
- âŒ **ë‹¨ì **: GPU ë©”ëª¨ë¦¬ í•„ìš”, ëª¨ë¸ í¬ê¸° ì œí•œ

### **Colab Pro API**
- âœ… **ì¥ì **: ê°•ë ¥í•œ GPU, ëŒ€ìš©ëŸ‰ ëª¨ë¸, ì‰¬ìš´ ì„¤ì •
- âŒ **ë‹¨ì **: ì¸í„°ë„· í•„ìš”, ë¹„ìš© ë°œìƒ, ì—°ê²° ë¶ˆì•ˆì •

---

## ğŸ¯ **ì¶”ì²œ ì„¤ì •**

### **ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©**
```bash
# ì‘ì€ ëª¨ë¸ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
LLM_MODEL=microsoft/DialoGPT-medium
```

### **í”„ë¡œë•ì…˜ìš©**
```bash
# ë¡œì»¬ ëŒ€ìš©ëŸ‰ ëª¨ë¸
LLM_MODEL=./model/local/korean-llama-2-7b
```

### **ê³ ì„±ëŠ¥ í•„ìš”ì‹œ**
```bash
# Colab Pro + ëŒ€ìš©ëŸ‰ ëª¨ë¸
COLAB_API_URL=https://your-colab-url.ngrok.io
LLM_MODEL=colab-api
```

---

## ğŸ” **ëª¨ë¸ ìƒíƒœ í™•ì¸**

### **í˜„ì¬ í™œì„± ëª¨ë¸ í™•ì¸**
```bash
curl http://localhost:8000/admin/model-info
```

### **ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡**
```bash
curl http://localhost:8000/admin/models
```

### **ëª¨ë¸ ì„±ëŠ¥ í†µê³„**
```bash
curl http://localhost:8000/analytics/model-stats
```

---

## âš ï¸ **ì£¼ì˜ì‚¬í•­**

### **ë¡œì»¬ ëª¨ë¸**
- **GPU ë©”ëª¨ë¦¬**: ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ 8GB~24GB í•„ìš”
- **ë””ìŠ¤í¬ ê³µê°„**: ëª¨ë¸ íŒŒì¼ í¬ê¸° í™•ì¸
- **ë¼ì´ì„ ìŠ¤**: ëª¨ë¸ ì‚¬ìš© ë¼ì´ì„ ìŠ¤ ì¤€ìˆ˜

### **Colab Pro**
- **ì„¸ì…˜ ì‹œê°„**: 12ì‹œê°„ ì œí•œ
- **ì—°ê²° ì•ˆì •ì„±**: ngrok ì¬ì—°ê²° í•„ìš”í•  ìˆ˜ ìˆìŒ
- **ë¹„ìš©**: Colab Pro êµ¬ë…ë£Œ

---

## ğŸš€ **ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ**

### **1. í•œêµ­ì–´ ëª¨ë¸ ì¶”ê°€**
```bash
# 1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
git clone https://huggingface.co/beomi/KoAlpaca-Polyglot-12.8B model/local/korean-alpaca

# 2. ì„¤ì • ì¶”ê°€
echo '{
  "models": {
    "korean-alpaca": {
      "name": "./model/local/korean-alpaca",
      "type": "local",
      "description": "í•œêµ­ì–´ Alpaca ëª¨ë¸",
      "language": "ko"
    }
  }
}' >> model/models.json

# 3. ëª¨ë¸ ì „í™˜
curl -X POST "http://localhost:8000/admin/switch-model" \
     -d '{"model_id": "korean-alpaca"}'
```

### **2. Colab Pro ì—°ê²°**
```bash
# 1. Colabì—ì„œ ngrok URL í™•ì¸
# 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export COLAB_API_URL=https://abc123.ngrok.io

# 3. ì—°ê²° í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8000/query" \
     -H "Content-Type: application/json" \
     -d '{"question": "ì•ˆë…•í•˜ì„¸ìš”"}'
```

**ì´ì œ ì›í•˜ëŠ” ëª¨ë¸ì„ ììœ ë¡­ê²Œ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ‰** 

# ğŸ¤– ì™¸ë¶€ ëª¨ë¸ ì—°ê²° ê°€ì´ë“œ

## ï¿½ï¿½ **ê°œìš”**

AI ë°±ì—”ë“œëŠ” **í˜ì´ì§€ë³„ ê³ ì • ëª¨ë¸ ì‹œìŠ¤í…œ**ì„ ì§€ì›í•©ë‹ˆë‹¤:

- **ì±„íŒ… í˜ì´ì§€** â†’ `my_colab_chat` ëª¨ë¸
- **ì½”ë”© í˜ì´ì§€** â†’ `my_colab_code` ëª¨ë¸  
- **ê±´ê°• í˜ì´ì§€** â†’ `health-expert` ëª¨ë¸
- **ì—¬í–‰ í˜ì´ì§€** â†’ `travel-expert` ëª¨ë¸
- **ë²•ë¥  í˜ì´ì§€** â†’ `legal-expert` ëª¨ë¸

---

## ğŸ¯ **í˜ì´ì§€ë³„ ê³ ì • ëª¨ë¸ ì‹œìŠ¤í…œ**

### **ì‘ë™ ì›ë¦¬**
```
ì‚¬ìš©ìê°€ health í˜ì´ì§€ì—ì„œ ì§ˆë¬¸ â†’ health-expert ëª¨ë¸ ìë™ ì„ íƒ
ì‚¬ìš©ìê°€ travel í˜ì´ì§€ì—ì„œ ì§ˆë¬¸ â†’ travel-expert ëª¨ë¸ ìë™ ì„ íƒ
ì‚¬ìš©ìê°€ chat í˜ì´ì§€ì—ì„œ ì§ˆë¬¸ â†’ my_colab_chat ëª¨ë¸ ìë™ ì„ íƒ
```

### **ì„¤ì • ë°©ë²•**
```json
// model/models.json
{
  "page_mapping": {
    "chat": "my_colab_chat",
    "code": "my_colab_code",
    "health": "health-expert", 
    "travel": "travel-expert",
    "legal": "legal-expert",
    "investment": "korean-llm"
  }
}
```

### **í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í˜ì´ì§€ ì •ë³´ ì „ì†¡**
```javascript
// health í˜ì´ì§€ì—ì„œ ì§ˆë¬¸í•  ë•Œ
const response = await fetch('http://localhost:8000/query', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
        question: "ê±´ê°•í•œ ìš´ë™ ë°©ë²•ì´ ê¶ê¸ˆí•´ìš”",
        user_id: 'user_123',
        context: { page: 'health' }  // â† ì´ê²Œ ì¤‘ìš”!
    })
});

// chat í˜ì´ì§€ì—ì„œ ì§ˆë¬¸í•  ë•Œ  
const response = await fetch('http://localhost:8000/query', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
        question: "ì•ˆë…•í•˜ì„¸ìš”",
        user_id: 'user_123',
        context: { page: 'chat' }  // â† ì´ê²Œ ì¤‘ìš”!
    })
});
```

---

## ğŸ”§ **ë°©ë²• 1: í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì • (ê°„ë‹¨)**

### **1ë‹¨ê³„: .env íŒŒì¼ ì„¤ì •**
```bash
# .env íŒŒì¼ì— ì¶”ê°€
CUSTOM_API_URL=https://your-model-api.com
CUSTOM_API_KEY=your_api_key_here
```

### **2ë‹¨ê³„: API ì„œë²„ ì¤€ë¹„**
ë‹¹ì‹ ì˜ API ì„œë²„ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤:

```python
# Flask/FastAPI ì˜ˆì‹œ
@app.post("/generate")
def generate_response():
    data = request.json
    prompt = data["prompt"]
    max_tokens = data.get("max_tokens", 512)
    temperature = data.get("temperature", 0.7)
    
    # ì—¬ê¸°ì„œ ëª¨ë¸ ì¶”ë¡ 
    response = your_model.generate(prompt)
    
    return {
        "response": response,
        "status": "success"
    }
```

### **3ë‹¨ê³„: ì„œë²„ ì¬ì‹œì‘**
```bash
cd backend
python main.py
```

---

## ğŸ›ï¸ **ë°©ë²• 2: ëª¨ë¸ ì„¤ì • íŒŒì¼ ë°©ì‹ (ê¶Œì¥)**

### **1ë‹¨ê³„: models.json ìˆ˜ì •**
```json
{
  "models": {
    "my_colab_chat": {
      "name": "my-chat-model",
      "type": "remote_api",
      "description": "ë‚´ Colab ì±„íŒ… ëª¨ë¸",
      "api_url": "https://abc123.ngrok.io",
      "api_key": "",
      "parameters": {
        "max_tokens": 512,
        "temperature": 0.7,
        "top_p": 0.9
      },
      "supported_tasks": ["text-generation", "chat"],
      "language": "ko",
      "domain": "general"
    },
    "my_colab_code": {
      "name": "my-code-model",
      "type": "remote_api",
      "description": "ë‚´ Colab ì½”ë“œ ìƒì„± ëª¨ë¸",
      "api_url": "https://def456.ngrok.io",
      "api_key": "",
      "parameters": {
        "max_tokens": 1024,
        "temperature": 0.1,
        "top_p": 0.9
      },
      "supported_tasks": ["text-generation", "code-generation"],
      "language": "ko",
      "domain": "coding"
    }
  },
  "active_model": "my_colab_chat",
  "page_mapping": {
    "chat": "my_colab_chat",
    "code": "my_colab_code"
  }
}
```

### **2ë‹¨ê³„: API ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„**

#### **ê¸°ë³¸ í˜•ì‹**
```python
@app.post("/generate")
def generate():
    data = request.json
    prompt = data["prompt"]
    
    # ëª¨ë¸ ì¶”ë¡ 
    response = your_model.generate(prompt)
    
    return {"response": response}
```

#### **OpenAI í˜¸í™˜ í˜•ì‹**
```python
@app.post("/v1/chat/completions")
def chat_completions():
    data = request.json
    messages = data["messages"]
    prompt = messages[-1]["content"]
    
    # ëª¨ë¸ ì¶”ë¡ 
    response = your_model.generate(prompt)
    
    return {
        "choices": [{
            "message": {
                "content": response
            }
        }]
    }
```

### **3ë‹¨ê³„: ëª¨ë¸ ì „í™˜**
```bash
# APIë¡œ ëª¨ë¸ ì „í™˜
curl -X POST "http://localhost:8000/admin/switch-model" \
  -H "Content-Type: application/json" \
  -d '{"model_id": "my_colab_chat"}'
```

---

## ğŸŒ **ë°©ë²• 3: Colab + ngrok ì„¤ì •**

### **1ë‹¨ê³„: Colabì—ì„œ ëª¨ë¸ ì„œë²„ ì‹¤í–‰**
```python
# Colab ë…¸íŠ¸ë¶
!pip install flask flask-cors

from flask import Flask, request, jsonify
from flask_cors import CORS
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

app = Flask(__name__)
CORS(app)

# ëª¨ë¸ ë¡œë“œ
model_name = "your-model-name"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

@app.post("/generate")
def generate():
    data = request.json
    prompt = data["prompt"]
    max_tokens = data.get("max_tokens", 512)
    temperature = data.get("temperature", 0.7)
    
    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(prompt, return_tensors="pt")
    
    # ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            inputs["input_ids"],
            max_new_tokens=max_tokens,
            temperature=temperature,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # ë””ì½”ë”©
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response[len(prompt):]  # í”„ë¡¬í”„íŠ¸ ì œê±°
    
    return {"response": response}

# ngrokìœ¼ë¡œ í„°ë„ë§
!pip install pyngrok
from pyngrok import ngrok

# ngrok í„°ë„ ìƒì„±
public_url = ngrok.connect(5000)
print(f"Public URL: {public_url}")

# ì„œë²„ ì‹¤í–‰
app.run(host="0.0.0.0", port=5000)
```

### **2ë‹¨ê³„: models.jsonì— URL ì¶”ê°€**
```json
{
  "models": {
    "colab_model": {
      "name": "colab-model",
      "type": "remote_api",
      "api_url": "https://abc123.ngrok.io",
      "parameters": {
        "max_tokens": 512,
        "temperature": 0.7
      },
      "domain": "general"
    }
  },
  "active_model": "colab_model"
}
```

---

## ğŸ” **API ì‘ë‹µ í˜•ì‹ ì§€ì›**

ë°±ì—”ë“œëŠ” **ë‹¤ì–‘í•œ ì‘ë‹µ í˜•ì‹**ì„ ì§€ì›í•©ë‹ˆë‹¤:

### **1. ê¸°ë³¸ í˜•ì‹**
```json
{
  "response": "ëª¨ë¸ì´ ìƒì„±í•œ í…ìŠ¤íŠ¸"
}
```

### **2. OpenAI í˜•ì‹**
```json
{
  "choices": [{
    "message": {
      "content": "ëª¨ë¸ì´ ìƒì„±í•œ í…ìŠ¤íŠ¸"
    }
  }]
}
```

### **3. ë‹¨ìˆœ í…ìŠ¤íŠ¸ í˜•ì‹**
```json
{
  "text": "ëª¨ë¸ì´ ìƒì„±í•œ í…ìŠ¤íŠ¸"
}
```

---

## ğŸ§ª **í…ŒìŠ¤íŠ¸ ë°©ë²•**

### **1. API ì§ì ‘ í…ŒìŠ¤íŠ¸**
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "ì•ˆë…•í•˜ì„¸ìš”",
    "user_id": "test_user",
    "context": {"domain": "general"}
  }'
```

### **2. ëª¨ë¸ ì •ë³´ í™•ì¸**
```bash
curl "http://localhost:8000/admin/model-info"
```

### **3. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡**
```bash
curl "http://localhost:8000/admin/models"
```

---

## âš ï¸ **ì£¼ì˜ì‚¬í•­**

### **1. API í‚¤ ë³´ì•ˆ**
- API í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜ë‚˜ ì„¤ì • íŒŒì¼ì— ì €ì¥
- Gitì— ì»¤ë°‹í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜
- í”„ë¡œë•ì…˜ì—ì„œëŠ” ë” ê°•ë ¥í•œ ë³´ì•ˆ ì ìš©

### **2. ì‘ë‹µ ì‹œê°„**
- ì›ê²© APIëŠ” ë„¤íŠ¸ì›Œí¬ ì§€ì—° ë°œìƒ
- íƒ€ì„ì•„ì›ƒ ì„¤ì • ê¶Œì¥ (30ì´ˆ)
- ì—ëŸ¬ ì²˜ë¦¬ êµ¬í˜„ í•„ìˆ˜

### **3. ë¹„ìš© ê´€ë¦¬**
- API í˜¸ì¶œ íšŸìˆ˜ ëª¨ë‹ˆí„°ë§
- ì‚¬ìš©ëŸ‰ ì œí•œ ì„¤ì •
- ë¹„ìš© ì•Œë¦¼ ì„¤ì •

---

## ğŸš€ **ê³ ê¸‰ ì„¤ì •**

### **ë„ë©”ì¸ë³„ ìë™ ëª¨ë¸ ì „í™˜**
```json
{
  "model_switching": {
    "enabled": true,
    "auto_switch_by_domain": true
  },
  "page_mapping": {
    "health": "health-expert",
    "travel": "travel-expert",
    "legal": "legal-expert",
    "coding": "my_colab_code"
  }
}
```

### **ë°°ì¹˜ ì²˜ë¦¬**
```python
# ì—¬ëŸ¬ ì§ˆë¬¸ì„ í•œ ë²ˆì— ì²˜ë¦¬
responses = await llm_service.batch_generate([
    "ì§ˆë¬¸ 1",
    "ì§ˆë¬¸ 2", 
    "ì§ˆë¬¸ 3"
])
```

ì´ì œ ë‹¹ì‹ ì˜ ëª¨ë¸ì„ ì‰½ê²Œ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ‰ 