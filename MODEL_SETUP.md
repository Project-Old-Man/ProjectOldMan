# ğŸ¤– AI ëª¨ë¸ ì„¤ì • ê°€ì´ë“œ

## ğŸ¯ **ëª¨ë¸ ì—°ê²° ë°©ë²• 2ê°€ì§€**

### **ë°©ë²• 1: ë¡œì»¬ ëª¨ë¸ íŒŒì¼ ì‚¬ìš©**
### **ë°©ë²• 2: Colab Pro API ì—°ê²°**

---

## ğŸ“ **ë°©ë²• 1: ë¡œì»¬ ëª¨ë¸ íŒŒì¼ ì‚¬ìš©**

### **1ë‹¨ê³„: ëª¨ë¸ íŒŒì¼ ì¤€ë¹„**

#### **ì§€ì›í•˜ëŠ” ëª¨ë¸ í˜•ì‹**
- âœ… **HuggingFace Transformers**: `.bin`, `.safetensors`
- âœ… **GGUF**: `.gguf` (vLLM 0.3.0+)
- âœ… **AWQ**: `.awq`
- âœ… **GPTQ**: `.gptq`

#### **ëª¨ë¸ íŒŒì¼ êµ¬ì¡°**
```
model/local/my-korean-model/
â”œâ”€â”€ config.json              # ëª¨ë¸ ì„¤ì •
â”œâ”€â”€ tokenizer.json           # í† í¬ë‚˜ì´ì €
â”œâ”€â”€ tokenizer_config.json    # í† í¬ë‚˜ì´ì € ì„¤ì •
â”œâ”€â”€ model.safetensors        # ëª¨ë¸ ê°€ì¤‘ì¹˜ (ëŒ€ìš©ëŸ‰)
â”œâ”€â”€ generation_config.json   # ìƒì„± ì„¤ì •
â””â”€â”€ README.md               # ëª¨ë¸ ì„¤ëª…
```

### **2ë‹¨ê³„: ëª¨ë¸ íŒŒì¼ ë³µì‚¬**
```bash
# ëª¨ë¸ íŒŒì¼ì„ model/local/ ë””ë ‰í† ë¦¬ì— ë³µì‚¬
cp -r /path/to/your/model model/local/my-korean-model

# ì˜ˆì‹œ: í•œêµ­ì–´ ëª¨ë¸
cp -r ~/Downloads/korean-llama-2-7b model/local/korean-llama
```

### **3ë‹¨ê³„: ëª¨ë¸ ì„¤ì • ì¶”ê°€**
```bash
# model/models.json íŒŒì¼ì— ëª¨ë¸ ì •ë³´ ì¶”ê°€
{
  "models": {
    "my-korean-model": {
      "name": "./model/local/my-korean-model",
      "type": "local",
      "description": "ë‚´ê°€ ë§Œë“  í•œêµ­ì–´ ëª¨ë¸",
      "parameters": {
        "max_tokens": 1024,
        "temperature": 0.7,
        "top_p": 0.9
      },
      "supported_tasks": ["text-generation", "chat"],
      "language": "ko",
      "domain": "general"
    }
  }
}
```

### **4ë‹¨ê³„: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**
```bash
# .env íŒŒì¼ì—ì„œ
LLM_MODEL=./model/local/my-korean-model
```

### **5ë‹¨ê³„: ì„œë²„ ì¬ì‹œì‘**
```bash
# ë°±ì—”ë“œ ì¬ì‹œì‘
cd backend
python3 -m uvicorn main:app --reload
```

---

## â˜ï¸ **ë°©ë²• 2: Colab Pro API ì—°ê²°**

### **1ë‹¨ê³„: Colab Proì—ì„œ ëª¨ë¸ ì„œë¹™**

#### **Colab ë…¸íŠ¸ë¶ì—ì„œ FastAPI ì„œë²„ ì‹¤í–‰**
```python
# Colab ë…¸íŠ¸ë¶ì— ë‹¤ìŒ ì½”ë“œ ì‘ì„±
!pip install fastapi uvicorn transformers torch

from fastapi import FastAPI
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

app = FastAPI()

# ëª¨ë¸ ë¡œë“œ
model_name = "your-model-name"  # ì˜ˆ: "microsoft/DialoGPT-medium"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

@app.post("/generate")
async def generate_text(request: dict):
    prompt = request["prompt"]
    temperature = request.get("temperature", 0.7)
    max_tokens = request.get("max_tokens", 512)
    
    # í† í°í™”
    inputs = tokenizer.encode(prompt, return_tensors="pt")
    
    # ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_length=inputs.shape[1] + max_tokens,
            temperature=temperature,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # ë””ì½”ë”©
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response[len(prompt):]  # í”„ë¡¬í”„íŠ¸ ì œê±°
    
    return {"response": response}

# ì„œë²„ ì‹¤í–‰
import uvicorn
uvicorn.run(app, host="0.0.0.0", port=8000)
```

#### **ngrokìœ¼ë¡œ ì™¸ë¶€ ì ‘ì† ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ê¸°**
```python
# Colabì—ì„œ ngrok ì„¤ì¹˜ ë° ì‹¤í–‰
!pip install pyngrok
from pyngrok import ngrok

# í„°ë„ ìƒì„±
public_url = ngrok.connect(8000)
print(f"Public URL: {public_url}")
```

### **2ë‹¨ê³„: ë°±ì—”ë“œì—ì„œ Colab API ì—°ê²°**

#### **í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**
```bash
# .env íŒŒì¼ì— ì¶”ê°€
COLAB_API_URL=https://your-ngrok-url.ngrok.io
```

#### **ëª¨ë¸ ì„¤ì • ì¶”ê°€**
```json
{
  "models": {
    "colab-model": {
      "name": "colab-api",
      "type": "colab",
      "description": "Colab Proì—ì„œ ì‹¤í–‰í•˜ëŠ” ëª¨ë¸",
      "parameters": {
        "max_tokens": 1024,
        "temperature": 0.7,
        "top_p": 0.9
      },
      "supported_tasks": ["text-generation", "chat"],
      "language": "ko",
      "domain": "general"
    }
  }
}
```

### **3ë‹¨ê³„: API ì—°ê²° í…ŒìŠ¤íŠ¸**
```bash
# Colab API í…ŒìŠ¤íŠ¸
curl -X POST "https://your-ngrok-url.ngrok.io/generate" \
     -H "Content-Type: application/json" \
     -d '{"prompt": "ì•ˆë…•í•˜ì„¸ìš”", "max_tokens": 100}'
```

---

## ğŸ”§ **ëª¨ë¸ ì „í™˜ ë°©ë²•**

### **APIë¡œ ëª¨ë¸ ì „í™˜**
```bash
# ë¡œì»¬ ëª¨ë¸ë¡œ ì „í™˜
curl -X POST "http://localhost:8000/admin/switch-model" \
     -H "Content-Type: application/json" \
     -d '{"model_id": "my-korean-model"}'

# Colab ëª¨ë¸ë¡œ ì „í™˜
curl -X POST "http://localhost:8000/admin/switch-model" \
     -H "Content-Type: application/json" \
     -d '{"model_id": "colab-model"}'
```

### **í™˜ê²½ ë³€ìˆ˜ë¡œ ì „í™˜**
```bash
# ë¡œì»¬ ëª¨ë¸
export LLM_MODEL=./model/local/my-korean-model

# Colab API
export LLM_MODEL=colab-api
```

---

## ğŸ“Š **ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ**

### **ë¡œì»¬ ëª¨ë¸**
- âœ… **ì¥ì **: ë¹ ë¥¸ ì‘ë‹µ, ì˜¤í”„ë¼ì¸ ì‘ë™, ë¹„ìš© ì—†ìŒ
- âŒ **ë‹¨ì **: GPU ë©”ëª¨ë¦¬ í•„ìš”, ëª¨ë¸ í¬ê¸° ì œí•œ

### **Colab Pro API**
- âœ… **ì¥ì **: ê°•ë ¥í•œ GPU, ëŒ€ìš©ëŸ‰ ëª¨ë¸, ì‰¬ìš´ ì„¤ì •
- âŒ **ë‹¨ì **: ì¸í„°ë„· í•„ìš”, ë¹„ìš© ë°œìƒ, ì—°ê²° ë¶ˆì•ˆì •

---

## ğŸ¯ **ì¶”ì²œ ì„¤ì •**

### **ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©**
```bash
# ì‘ì€ ëª¨ë¸ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
LLM_MODEL=microsoft/DialoGPT-medium
```

### **í”„ë¡œë•ì…˜ìš©**
```bash
# ë¡œì»¬ ëŒ€ìš©ëŸ‰ ëª¨ë¸
LLM_MODEL=./model/local/korean-llama-2-7b
```

### **ê³ ì„±ëŠ¥ í•„ìš”ì‹œ**
```bash
# Colab Pro + ëŒ€ìš©ëŸ‰ ëª¨ë¸
COLAB_API_URL=https://your-colab-url.ngrok.io
LLM_MODEL=colab-api
```

---

## ğŸ” **ëª¨ë¸ ìƒíƒœ í™•ì¸**

### **í˜„ì¬ í™œì„± ëª¨ë¸ í™•ì¸**
```bash
curl http://localhost:8000/admin/model-info
```

### **ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡**
```bash
curl http://localhost:8000/admin/models
```

### **ëª¨ë¸ ì„±ëŠ¥ í†µê³„**
```bash
curl http://localhost:8000/analytics/model-stats
```

---

## âš ï¸ **ì£¼ì˜ì‚¬í•­**

### **ë¡œì»¬ ëª¨ë¸**
- **GPU ë©”ëª¨ë¦¬**: ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ 8GB~24GB í•„ìš”
- **ë””ìŠ¤í¬ ê³µê°„**: ëª¨ë¸ íŒŒì¼ í¬ê¸° í™•ì¸
- **ë¼ì´ì„ ìŠ¤**: ëª¨ë¸ ì‚¬ìš© ë¼ì´ì„ ìŠ¤ ì¤€ìˆ˜

### **Colab Pro**
- **ì„¸ì…˜ ì‹œê°„**: 12ì‹œê°„ ì œí•œ
- **ì—°ê²° ì•ˆì •ì„±**: ngrok ì¬ì—°ê²° í•„ìš”í•  ìˆ˜ ìˆìŒ
- **ë¹„ìš©**: Colab Pro êµ¬ë…ë£Œ

---

## ğŸš€ **ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ**

### **1. í•œêµ­ì–´ ëª¨ë¸ ì¶”ê°€**
```bash
# 1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
git clone https://huggingface.co/beomi/KoAlpaca-Polyglot-12.8B model/local/korean-alpaca

# 2. ì„¤ì • ì¶”ê°€
echo '{
  "models": {
    "korean-alpaca": {
      "name": "./model/local/korean-alpaca",
      "type": "local",
      "description": "í•œêµ­ì–´ Alpaca ëª¨ë¸",
      "language": "ko"
    }
  }
}' >> model/models.json

# 3. ëª¨ë¸ ì „í™˜
curl -X POST "http://localhost:8000/admin/switch-model" \
     -d '{"model_id": "korean-alpaca"}'
```

### **2. Colab Pro ì—°ê²°**
```bash
# 1. Colabì—ì„œ ngrok URL í™•ì¸
# 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export COLAB_API_URL=https://abc123.ngrok.io

# 3. ì—°ê²° í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8000/query" \
     -H "Content-Type: application/json" \
     -d '{"question": "ì•ˆë…•í•˜ì„¸ìš”"}'
```

**ì´ì œ ì›í•˜ëŠ” ëª¨ë¸ì„ ììœ ë¡­ê²Œ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ‰** 