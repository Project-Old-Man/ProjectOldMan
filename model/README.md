# ëª¨ë¸ ë””ë ‰í† ë¦¬

ì´ ë””ë ‰í† ë¦¬ëŠ” AI ëª¨ë¸ íŒŒì¼ë“¤ì„ ì €ì¥í•˜ëŠ” ê³³ì…ë‹ˆë‹¤.

## ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
model/
â”œâ”€â”€ README.md                    # ì´ íŒŒì¼
â”œâ”€â”€ .gitkeep                     # ë¹ˆ ë””ë ‰í† ë¦¬ ìœ ì§€ìš©
â”œâ”€â”€ models.json                  # ëª¨ë¸ ì„¤ì • íŒŒì¼
â”œâ”€â”€ local/                       # ë¡œì»¬ ëª¨ë¸ íŒŒì¼ë“¤
â”‚   â”œâ”€â”€ korean-llm/             # í•œêµ­ì–´ LLM ëª¨ë¸
â”‚   â”œâ”€â”€ health-expert/          # ê±´ê°• ì „ë¬¸ ëª¨ë¸
â”‚   â”œâ”€â”€ travel-expert/          # ì—¬í–‰ ì „ë¬¸ ëª¨ë¸
â”‚   â””â”€â”€ legal-expert/           # ë²•ë¥  ì „ë¬¸ ëª¨ë¸
â””â”€â”€ configs/                    # ëª¨ë¸ ì„¤ì • íŒŒì¼ë“¤
    â”œâ”€â”€ korean-llm.json
    â”œâ”€â”€ health-expert.json
    â””â”€â”€ travel-expert.json
```

## ğŸš€ ëª¨ë¸ ì¶”ê°€ ë°©ë²•

### 1. ë¡œì»¬ ëª¨ë¸ íŒŒì¼ ì¶”ê°€

```bash
# ëª¨ë¸ íŒŒì¼ì„ model/local/ ë””ë ‰í† ë¦¬ì— ë³µì‚¬
cp -r /path/to/your/model model/local/your-model-name
```

### 2. ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±

`model/configs/your-model-name.json` íŒŒì¼ì„ ìƒì„±:

```json
{
  "name": "your-model-name",
  "type": "local",
  "path": "./model/local/your-model-name",
  "description": "ëª¨ë¸ ì„¤ëª…",
  "parameters": {
    "max_tokens": 512,
    "temperature": 0.7,
    "top_p": 0.9
  },
  "supported_tasks": ["text-generation", "chat"],
  "language": "ko",
  "domain": "general"
}
```

### 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env` íŒŒì¼ì—ì„œ ëª¨ë¸ ê²½ë¡œ ì„¤ì •:

```bash
# ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©ì‹œ
LLM_MODEL=./model/local/your-model-name

# ë˜ëŠ” ì ˆëŒ€ ê²½ë¡œ
LLM_MODEL=/app/model/local/your-model-name
```

## ğŸ“‹ ì§€ì›í•˜ëŠ” ëª¨ë¸ í˜•ì‹

### vLLM ì§€ì› í˜•ì‹
- **HuggingFace Transformers**: `.bin`, `.safetensors`
- **GGUF**: `.gguf` (vLLM 0.3.0+)
- **AWQ**: `.awq`
- **GPTQ**: `.gptq`

### ëª¨ë¸ íŒŒì¼ ì˜ˆì‹œ
```
model/local/korean-llm/
â”œâ”€â”€ config.json
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer_config.json
â”œâ”€â”€ model.safetensors
â””â”€â”€ generation_config.json
```

## ğŸ”§ ëª¨ë¸ ì „í™˜ ë°©ë²•

### 1. í™˜ê²½ ë³€ìˆ˜ë¡œ ì „í™˜
```bash
# í•œêµ­ì–´ ëª¨ë¸
export LLM_MODEL=./model/local/korean-llm

# ê±´ê°• ì „ë¬¸ ëª¨ë¸
export LLM_MODEL=./model/local/health-expert

# ì„œë²„ ì¬ì‹œì‘
docker-compose restart backend
```

### 2. APIë¡œ ëª¨ë¸ ì „í™˜
```bash
curl -X POST "http://localhost:8000/admin/switch-model" \
     -H "Content-Type: application/json" \
     -d '{"model_path": "./model/local/korean-llm"}'
```

## ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

ëª¨ë¸ ì‚¬ìš© í†µê³„ëŠ” ë‹¤ìŒ APIë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# ëª¨ë¸ ì •ë³´ ì¡°íšŒ
curl http://localhost:8000/admin/model-info

# ì„±ëŠ¥ í†µê³„
curl http://localhost:8000/analytics/model-stats
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ëª¨ë¸ í¬ê¸°**: ëŒ€ìš©ëŸ‰ ëª¨ë¸ì€ ì¶©ë¶„í•œ ë””ìŠ¤í¬ ê³µê°„ í•„ìš”
2. **ë©”ëª¨ë¦¬**: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
3. **ë¼ì´ì„ ìŠ¤**: ëª¨ë¸ ì‚¬ìš© ë¼ì´ì„ ìŠ¤ ì¤€ìˆ˜
4. **ë°±ì—…**: ì¤‘ìš”í•œ ëª¨ë¸ íŒŒì¼ì€ ë°±ì—… ë³´ê´€

## ğŸ†˜ ë¬¸ì œ í•´ê²°

### ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨
```bash
# ë¡œê·¸ í™•ì¸
docker-compose logs backend

# ëª¨ë¸ ê²½ë¡œ í™•ì¸
ls -la model/local/your-model-name/
```

### ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
nvidia-smi

# í™˜ê²½ ë³€ìˆ˜ ì¡°ì •
export GPU_MEMORY_UTILIZATION=0.7
``` 